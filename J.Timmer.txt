On Monday, Nature released a paper from Google's quantum computing team that provides a key demonstration of the potential of quantum error correction. Thanks to an improved processor, Google's team found that increasing the number of hardware qubits dedicated to an error-corrected logical qubit led to an exponential increase in performance. By the time the entire 105-qubit processor was dedicated to hosting a single error-corrected qubit, the system was stable for an average of an hour.


In fact, Google told Ars that errors on this single logical qubit were rare enough that it was difficult to study them. The work provides a significant validation that quantum error correction is likely to be capable of supporting the execution of complex algorithms that might require hours to execute.


A new fab
Google is making a number of announcements in association with the paper's release (an earlier version of the paper has been up on the arXiv since August). One of those is that the company is committed enough to its quantum computing efforts that it has built its own fabrication facility for its superconducting processors.


"In the past, all the Sycamore devices that you've heard about were fabricated in a shared university clean room space next to graduate students and people doing kinds of crazy stuff," Google's Julian Kelly said. "And we've made this really significant investment in bringing this new facility online, hiring staff, filling it with tools, transferring their process over. And that enables us to have significantly more process control and dedicated tooling."


Ars Video
How Scientists Respond to Science Deniers


That's likely to be a critical step for the company, as the ability to fabricate smaller test devices can allow the exploration of lots of ideas on how to structure the hardware to limit the impact of noise. The first publicly announced product of this lab is the Willow processor, Google's second design, which ups its qubit count to 105. Kelly said one of the changes that came with Willow actually involved making the individual pieces of the qubit larger, which makes them somewhat less susceptible to the influence of noise.


Recently, Congress' Select Subcommittee on the Coronavirus Pandemic released its final report. The basic gist is about what you'd expect from a Republican-run committee, in that it trashes a lot of Biden-era policies and state-level responses while praising a number of Trump's decisions. But what's perhaps most striking is how it tackles a variety of scientific topics, including many where there's a large, complicated body of evidence.


Notably, this includes conclusions about the origin of the pandemic, which the report describes as "most likely" emerging from a lab rather than being the product of the zoonotic transfer between an animal species and humans. The latter explanation is favored by many scientists.


The conclusions themselves aren't especially interesting; they're expected from a report with partisan aims. But the method used to reach those conclusions is often striking: The Republican majority engages in a process of systematically changing the standard of evidence needed for it to reach a conclusion. For a conclusion the report's authors favor, they'll happily accept evidence from computer models or arguments from an editorial in the popular press; for conclusions they disfavor, they demand double-blind controlled clinical trials.


This approach, which I'll term "shifting the evidentiary baseline," shows up in many arguments regarding scientific evidence. But it has rarely been employed quite this pervasively. So let's take a look at it in some detail and examine a few of the other approaches the report uses to muddy the waters regarding science. We're likely to see many of them put to use in the near future.


By some measures, AI systems are now competitive with traditional computing methods for generating weather forecasts. Because their training penalizes errors, however, the forecasts tend to get "blurry"—as you move further ahead in time, the models make fewer specific predictions since those are more likely to be wrong. As a result, you start to see things like storm tracks broadening and the storms themselves losing clearly defined edges.


But using AI is still extremely tempting because the alternative is a computational atmospheric circulation model, which is extremely compute-intensive. Still, it's highly successful, with the ensemble model from the European Centre for Medium-Range Weather Forecasts considered the best in class.


In a paper being released today, Google's DeepMind claims its new AI system manages to outperform the European model on forecasts out to at least a week and often beyond. DeepMind's system, called GenCast, merges some computational approaches used by atmospheric scientists with a diffusion model, commonly used in generative AI. The result is a system that maintains high resolution while cutting the computational cost significantly.


Ensemble forecasting
Traditional computational methods have two main advantages over AI systems. The first is that they're directly based on atmospheric physics, incorporating the rules we know govern the behavior of our actual weather, and they calculate some of the details in a way that's directly informed by empirical data. They're also run as ensembles, meaning that multiple instances of the model are run. Due to the chaotic nature of the weather, these different runs will gradually diverge, providing a measure of the uncertainty of the forecast.


At least one attempt has been made to merge some of the aspects of traditional weather models with AI systems. An internal Google project used a traditional atmospheric circulation model that divided the Earth's surface into a grid of cells but used an AI to predict the behavior of each cell. This provided much better computational performance, but at the expense of relatively large grid cells, which resulted in relatively low resolution.


For its take on AI weather predictions, DeepMind decided to skip the physics and instead adopt the ability to run an ensemble.


Gen Cast is based on diffusion models, which have a key feature that's useful here. In essence, these models are trained by starting them with a mixture of an original—image, text, weather pattern—and then a variation where noise is injected. The system is supposed to create a variation of the noisy version that is closer to the original. Once trained, it can be fed pure noise and evolve the noise to be closer to whatever it's targeting.


In this case, the target is realistic weather data, and the system takes an input of pure noise and evolves it based on the atmosphere's current state and its recent history. For longer-range forecasts, the "history" includes both the actual data and the predicted data from earlier forecasts. The system moves forward in 12-hour steps, so the forecast for day three will incorporate the starting conditions, the earlier history, and the two forecasts from days one and two.


This is useful for creating an ensemble forecast because you can feed it different patterns of noise as input, and each will produce a slightly different output of weather data. This serves the same purpose it does in a traditional weather model: providing a measure of the uncertainty for the forecast.


For each grid square, GenCast works with six weather measures at the surface, along with six that track the state of the atmosphere and 13 different altitudes at which it estimates the air pressure. Each of these grid squares is 0.2 degrees on a side, a higher resolution than the European model uses for its forecasts. Despite that resolution, DeepMind estimates that a single instance (meaning not a full ensemble) can be run out to 15 days on one of Google's tensor processing systems in just eight minutes.


It's possible to make an ensemble forecast by running multiple versions of this in parallel and then integrating the results. Given the amount of hardware Google has at its disposal, the whole process from start to finish is likely to take less than 20 minutes. The source and training data will be placed on the GitHub page for DeepMind's GraphCast project. Given the relatively low computational requirements, we can probably expect individual academic research teams to start experimenting with it.


Measures of success
DeepMind reports that GenCast dramatically outperforms the best traditional forecasting model. Using a standard benchmark in the field, DeepMind found that GenCast was more accurate than the European model on 97 percent of the tests it used, which checked different output values at different times in the future. In addition, the confidence values, based on the uncertainty obtained from the ensemble, were generally reasonable.


Past AI weather forecasters, having been trained on real-world data, are generally not great at handling extreme weather since it shows up so rarely in the training set. But GenCast did quite well, often outperforming the European model in things like abnormally high and low temperatures and air pressure (one percent frequency or less, including at the 0.01 percentile).


DeepMind also went beyond standard tests to determine whether GenCast might be useful. This research included projecting the tracks of tropical cyclones, an important job for forecasting models. For the first four days, GenCast was significantly more accurate than the European model, and it maintained its lead out to about a week.


One of DeepMind's most interesting tests was checking the global forecast of wind power output based on information from the Global Powerplant Database. This involved using it to forecast wind speeds at 10 meters above the surface (which is actually lower than where most turbines reside but is the best approximation possible) and then using that number to figure out how much power would be generated. The system beat the traditional weather model by 20 percent for the first two days and stayed in front with a declining lead out to a week.


The researchers don't spend much time examining why performance seems to decline gradually for about a week. Ideally, more details about GenCast's limitations would help inform further improvements, so the researchers are likely thinking about it. In any case, today's paper marks the second case where taking something akin to a hybrid approach—mixing aspects of traditional forecast systems with AI—has been reported to improve forecasts. And both those cases took very different approaches, raising the prospect that it will be possible to combine some of their features.


2023 was always going to be a hot year, given that warmer El Niño conditions were superimposed on the long-term trend of climate change driven by our greenhouse gas emissions. But it's not clear anybody was expecting the striking string of hot months that allowed the year to easily eclipse any previous year on record. As the warmth has continued at record levels even after the El Niño faded, it's an event that seems to demand an explanation.


On Thursday, a group of German scientists—Helge Goessling, Thomas Rackow, and Thomas Jung—released a paper that attempts to provide one. They present data that suggests the Earth is absorbing more incoming sunlight than it has in the past, largely due to reduced cloud cover.


Balancing the numbers on radiation
Years with strong El Niño conditions tend to break records. But the 2023 El Niño was relatively mild. The effects of the phenomenon are also directly felt in the tropical Pacific, yet ocean temperatures set records in the Atlantic and contributed to a massive retreat in ice near Antarctica. So, there are clearly limits to what can be attributed to El Niño. Other influences that have been considered include the injection of water vapor into the stratosphere by the Hunga Tonga eruption, and a reduction in sulfur emissions due to new rules governing international shipping. 2023 also corresponds to a peak in the most recent solar cycle.


Ars Video
How Lighting Design In The Callisto Protocol Elevates The Horror


But it is estimated that all three factors combined add up to, at most, 0.1 Kelvin of warming on top of the El Niño, which leaves an estimated 0.2 Kelvin of additional warming unaccounted for. The authors trace the difference back to an energy imbalance measured at the top of the atmosphere.


On Friday, the US Department of Agriculture (USDA) announced that it would begin a nationwide testing program for the presence of the H5N1 flu virus, also known as the bird flu. Testing will focus on pre-pasteurized milk at dairy processing facilities (pasteurization inactivates the virus), but the order that's launching the program will require anybody involved with milk production before then to provide samples to the USDA on request. That includes "any entity responsible for a dairy farm, bulk milk transporter, bulk milk transfer station, or dairy processing facility."


The ultimate goal is to identify individual herds where the virus is circulating and use the agency's existing powers to do contact tracing and restrict the movement of cattle, with the ultimate goal of eliminating the virus from US herds.


A bovine disease vector
At the time of publication, the CDC had identified 58 cases of humans infected by the H5N1 flu virus, over half of them in California. All but two have come about due to contact with agriculture, either cattle (35 cases) or poultry (21). The virus's genetic material has appeared in the milk supply and, although pasteurization should eliminate any intact infectious virus, raw milk is notable for not undergoing pasteurization, which has led to at least one recall when the virus made its way into raw milk. And we know the virus can spread to other species if they drink milk from infected cows.


Ars Video
What Happens to the Developers When AI Can Code? | Ars Frontiers


So far, the human H5N1 cases have generally been mild. But the worry is that prolonged circulation in other mammals may allow the virus to evolve in ways that will put humans at greater risk of infections or enable the infection to cause more severe symptoms.






So, the ultimate goal of the USDA is to eliminate cattle as a reservoir. When the Agency announced it was planning for this program, it noted that there were two candidate vaccines in trials. Until those are validated, it plans to use the standard playbook for handling emerging infections: contact tracing and isolation. And it has the ability to compel cattle and their owners to be more cooperative than the human population turned out to be.


The five-step plan
The USDA refers to isolation and contact tracing as Stage 3 of a five-stage plan for controlling H5N1 in cattle, with the two earlier stages being the mandatory sampling and testing, meant to be handled on a state-by-state basis. Following the successful containment of the virus in a state, the USDA will move on to batch sampling to ensure each state remains virus-free. This is essential, given that we don't have a clear picture of how many times the virus has jumped from its normal reservoir in birds into the cattle population.


That makes it possible that reaching Stage 5, which the USDA terms "Demonstrating Freedom from H5 in US Dairy Cattle," will turn out to be impossible. Dairy cattle are likely to have daily contact with birds, and it may be that the virus will be regularly re-introduced into the population, leaving containment as the only option until the vaccines are ready.


Testing will initially focus primarily on states where cattle-to-human transmission is known to have occurred or the virus is known to be present: California, Colorado, Michigan, Mississippi, Oregon, and Pennsylvania. If you wish to track the progress of the USDA's efforts, it will be posting weekly updates.